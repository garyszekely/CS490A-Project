{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from final_report import tokenize_doc, mask_tokens, train_mle, train_laplace, train_lidstone, train_stupid_backoff, test_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "\n",
    "Word prediction recently has become a very integral part of everyday lives as mobile devices become more accessible to people everyday. In this paper we will tackle the topic of building such a piece of software to predict words so we can deepen our understanding of the technology and see how it can be beneficial to the everyday person as well as to the field of augmentative and alternative communications (AAC). We used two models to test our predictor, an N-Grams based model and a LSTM based model. These our integral to our approach since we need models that can \"remember\" words that previously came before the word currently being predicted. We use this information to then calculate probabilities for the next possible word until the end of a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Related Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create our prediction models, we used a collection of 100,000 Amazon reviews across a multitide of products and reviewers gathered by the Stanford Network Analysis Project. Each review included it's polarity (positive or negative), a title, and the text of the review. The polarity of each review was determined through the star ratings given, 1 and 2 being negative with 4 and 5 being positive. Reviews of 3 stars are not included.\n",
    "\n",
    "Link: https://www.kaggle.com/kritanjalijain/amazon-reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization & Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization & Normalization\n",
    "\n",
    "Tokenization and normalization is very important as we need to ensure that our NLP model is not skewed by unclean data. Our first step in tokenization is to separate our training Amazon reviews into a list of sentence tokens using NLTK's sentence tokenizer. Finally, separate this list of sentence tokens into a list of lists of word tokens using NLTK's TweetTokenizer. As for normalization, we chose to lowercase all the word tokens as to get more meaningful results not altered by capitalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing and normalizing training data... Done\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Joe/nltk_data'\n    - 'C:\\\\Users\\\\Joe\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\Joe\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Joe\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Joe\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-ee01e05cb328>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Tokenizing and normalizing...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcsv_reader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0msentence_tokens\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m     \"\"\"\n\u001b[1;32m--> 107\u001b[1;33m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tokenizers/punkt/{0}.pickle\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    748\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    749\u001b[0m     \u001b[1;31m# Load the resource.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 750\u001b[1;33m     \u001b[0mopened_resource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    751\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    752\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"raw\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    873\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    874\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"nltk\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 875\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    876\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"file\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    877\u001b[0m         \u001b[1;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"*\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Joe/nltk_data'\n    - 'C:\\\\Users\\\\Joe\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\Joe\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Joe\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Joe\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "sentence_tokens = []\n",
    "with open('../data/sample_train.csv') as csv_file:\n",
    "    print('Tokenizing and normalizing training data...', end=' ')\n",
    "    sentence_tokens = tokenize_doc(csv_file)\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Training Sentence Tokens: 165696\n",
      "Average Number of Training Word Tokens per Training Sentence Token: 18.35504779837775\n",
      "Example Training Sentence Token: ['this', 'sound', 'track', 'was', 'beautiful', '!']\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "print('Total Training Sentence Tokens:', len(sentence_tokens))\n",
    "print('Average Number of Training Word Tokens per Training Sentence Token:', statistics.mean(map(lambda e: len(e), sentence_tokens)))\n",
    "print('Example Training Sentence Token:', sentence_tokens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Gram Models\n",
    "\n",
    "To begin, we first chose to model our problem uaing several N-Gram models. This is a rather simple approach to word prediction since we can store an arbitrary number of N-Grams and predict words based on the previous N - 1 words. To be more specific, we limited our N-Gram models to trigrams, bigrams, and unigrams which means we can predict words based on 2 or less previous words. The predicted word is the N-Gram that has the highest probability with that word as the last item and the previous items being the previous words, if any. The creation and preparation of such N-Grams for our models can be handled by NLTK's Everygram Preprocessor. The models come from NLTK's LanguageModels which we decided to use because of the ease of use in terms of input and output for our tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Likelihood Estimator (MLE)\n",
    "\n",
    "The MLE model serves as the basis for our N-Gram modeling. It utilizes the algorithm described above without any smoothing or extra preparation. The model's only concern is the raw likelihoods for the word predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MLE model... Done\n"
     ]
    }
   ],
   "source": [
    "print('Training MLE model...', end=' ')\n",
    "mle = train_mle(sentence_tokens)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laplace\n",
    "\n",
    "The Laplace model utilizes the MLE model while also implemeting add-1 smoothing. This leads to more accurate word probabilities in general since we can assign non-zero probabilities to unseen words. We are using this model as a direct comparison to the MLE model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Laplace model... Done\n"
     ]
    }
   ],
   "source": [
    "print('Training Laplace model...', end=' ')\n",
    "laplace = train_laplace(sentence_tokens)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lidstone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Lidstone model is the same as the Laplace model but instead of add-1 smoothing, we can specify the amount of smoothing. We chose to create three different Lidstone models, initialized with add-0.25 smoothing, add-0.5 smoothing, and add-0.75 smoothing, respectively. We are using this model to demonstrate how smoothing affects the word predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Lidstone models...\n"
     ]
    }
   ],
   "source": [
    "print('Training Lidstone models...')\n",
    "print('Training Lidstone (gamma=0.25) model...', end=' ')\n",
    "lidstone_25 = train_lidstone(sentence_tokens, 0.25)\n",
    "print('Done')\n",
    "print('Training Lidstone (gamma=0.5) model...', end=' ')\n",
    "lidstone_50 = train_lidstone(sentence_tokens, 0.5)\n",
    "print('Done')\n",
    "print('Training Lidstone (gamma=0.75) model...', end=' ')\n",
    "lidstone_75 = train_lidstone(sentence_tokens, 0.75)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stupid Backoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Stupid Backoff model utilizes the MLE model while also providing the ability to scale lower order probabilities. The downside of this is that it is not a true probability distribution. We chose to create three different Stupid Backoff models, initialized with 0.25, 0.5, and 0.75, respectively. We are using this model to determine at what degree lower order probabilities affect the word predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Stupid Backoff models...\n"
     ]
    }
   ],
   "source": [
    "print('Training Stupid Backoff models...')\n",
    "print('Training Stupid Backoff (alpha=0.25) model...', end=' ')\n",
    "stupid_backoff_25 = train_stupid_backoff(sentence_tokens, 0.25)\n",
    "print('Done')\n",
    "print('Training Stupid Backoff (alpha=0.25) model...', end=' ')\n",
    "stupid_backoff_50 = train_stupid_backoff(sentence_tokens, 0.5)\n",
    "print('Done')\n",
    "print('Training Stupid Backoff (alpha=0.25) model...', end=' ')\n",
    "stupid_backoff_75 = train_stupid_backoff(sentence_tokens, 0.75)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing and normalizing testing data...\n",
      "Masking word tokens in each sentence token...\n"
     ]
    }
   ],
   "source": [
    "sentence_tokens = []\n",
    "with open('../data/sample_test.csv') as csv_file:\n",
    "    print('Tokenizing and normalizing testing data...', end=' ')\n",
    "    sentence_tokens = tokenize_doc(csv_file)\n",
    "    print('Done')\n",
    "\n",
    "print('Masking word tokens in each sentence token...', end=' ')\n",
    "masked_sentence_tokens, masked_words = mask_tokens(sentence_tokens)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Gram Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Likelihood Estimator (MLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing MLE model...\n",
      "MLE Exact Prediction Accuracy: 8.74894336432798 %\n"
     ]
    }
   ],
   "source": [
    "print('Testing MLE model...', end=' ')\n",
    "mle_exact_accuracy = test_model(mle, masked_sentence_tokens, masked_words)\n",
    "print('Done')\n",
    "print('MLE Exact Prediction Accuracy:', mle_exact_accuracy, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Laplace model...\n",
      "Laplace Exact Prediction Accuracy: 7.6923076923076925 %\n"
     ]
    }
   ],
   "source": [
    "print('Testing Laplace model...', end=' ')\n",
    "laplace_exact_accuracy = test_model(laplace, masked_sentence_tokens, masked_words)\n",
    "print('Done')\n",
    "print('Laplace Exact Prediction Accuracy:', laplace_exact_accuracy, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lidstone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Lidstone models...\n",
      "Testing Lidstone (gamma=0.25) model...\n",
      "Testing Lidstone (gamma=0.50) model...\n",
      "Testing Lidstone (gamma=0.75) model...\n",
      "Lidstone (gamma=0.25) Exact Prediction Accuracy: 8.368554522400675 %\n",
      "Lidstone (gamma=0.50) Exact Prediction Accuracy: 7.6923076923076925 %\n",
      "Lidstone (gamma=0.75) Exact Prediction Accuracy: 8.241758241758241 %\n"
     ]
    }
   ],
   "source": [
    "print('Testing Lidstone models...')\n",
    "print('Testing Lidstone (gamma=0.25) model...', end=' ')\n",
    "lidstone_25_exact_accuracy = test_model(lidstone_25, masked_sentence_tokens, masked_words)\n",
    "print('Done')\n",
    "print('Testing Lidstone (gamma=0.50) model...', end=' ')\n",
    "lidstone_50_exact_accuracy = test_model(lidstone_50, masked_sentence_tokens, masked_words)\n",
    "print('Done')\n",
    "print('Testing Lidstone (gamma=0.75) model...', end=' ')\n",
    "lidstone_75_exact_accuracy = test_model(lidstone_75, masked_sentence_tokens, masked_words)\n",
    "print('Done')\n",
    "print('Lidstone (gamma=0.25) Exact Prediction Accuracy:', lidstone_25_exact_accuracy, '%')\n",
    "print('Lidstone (gamma=0.50) Exact Prediction Accuracy:', lidstone_50_exact_accuracy, '%')\n",
    "print('Lidstone (gamma=0.75) Exact Prediction Accuracy:', lidstone_75_exact_accuracy, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stupid Backoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Stupid Backoff models...\n",
      "Testing Studid Backoff (alpha=0.25) model...\n",
      "Testing Stupid Backoff (alpha=0.50) model...\n",
      "Testing Stupid Backoff (alpha=0.75) model...\n",
      "Stupid Backoff (alpha=0.25) Exact Prediction Accuracy: 9.171597633136095 %\n",
      "Stupid Backoff (alpha=0.50) Exact Prediction Accuracy: 7.734573119188504 %\n",
      "Stupid Backoff (alpha=0.75) Exact Prediction Accuracy: 9.129332206255283 %\n"
     ]
    }
   ],
   "source": [
    "print('Testing Stupid Backoff models...')\n",
    "print('Testing Studid Backoff (alpha=0.25) model...', end=' ')\n",
    "stupid_backoff_25_exact_accuracy = test_model(stupid_backoff_25, masked_sentence_tokens, masked_words)\n",
    "print('Done')\n",
    "print('Testing Stupid Backoff (alpha=0.50) model...', end=' ')\n",
    "stupid_backoff_50_exact_accuracy = test_model(stupid_backoff_50, masked_sentence_tokens, masked_words)\n",
    "print('Done')\n",
    "print('Testing Stupid Backoff (alpha=0.75) model...', end=' ')\n",
    "stupid_backoff_75_exact_accuracy = test_model(stupid_backoff_75, masked_sentence_tokens, masked_words)\n",
    "print('Done')\n",
    "print('Stupid Backoff (alpha=0.25) Exact Prediction Accuracy:', stupid_backoff_25_exact_accuracy, '%')\n",
    "print('Stupid Backoff (alpha=0.50) Exact Prediction Accuracy:', stupid_backoff_50_exact_accuracy, '%')\n",
    "print('Stupid Backoff (alpha=0.75) Exact Prediction Accuracy:', stupid_backoff_75_exact_accuracy, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion & Future Work"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
