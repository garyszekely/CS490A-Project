{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word prediction recently has become a very integral part of everyday lives as mobile devices become more accessible to people everyday. In this paper we will tackle the topic of building such a piece of software to predict words so we can deepen our understanding of the technology and see how it can be beneficial to the everyday person as well as to the field of augmentative and alternative communications (AAC). We used two models to test our predictor, an N-Grams based model and a LSTM based model. These our integral to our approach since we need models that can \"remember\" words that previously came before the word currently being predicted. We use this information to then calculate probabilities for the next possible word until the end of a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Related Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization & Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization and normalization is very important as we need to ensure that our NLP model is not skewed by unclean data. Our first step in tokenization is to separate our training Amazon reviews into a list of sentence tokens using NLTK's sentence tokenizer. Finally, separate this list of sentence tokens into a list of lists of word tokens using NLTK's TweetTokenizer. As for normalization, we chose to lowercase all the word tokens as to get more meaningful results not altered by capitalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing and normalizing...\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import csv\n",
    "\n",
    "sentence_tokens = []\n",
    "\n",
    "with open('../data/sample.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    print(\"Tokenizing and normalizing...\")\n",
    "    for row in csv_reader:\n",
    "        sentence_tokens += [nltk.word_tokenize(sentence.lower()) for sentence in nltk.sent_tokenize(row[2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Training Sentence Tokens: 470931\n",
      "Average Number of Training Word Tokens per Training Sentence Token: 18.113396654711625\n",
      "Example Training Sentence Token: ['my', 'lovely', 'pat', 'has', 'one', 'of', 'the', 'great', 'voices', 'of', 'her', 'generation', '.']\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "print(\"Total Training Sentence Tokens:\", len(sentence_tokens))\n",
    "print(\"Average Number of Training Word Tokens per Training Sentence Token:\", statistics.mean(map(lambda e: len(e), sentence_tokens)))\n",
    "print(\"Example Training Sentence Token:\", sentence_tokens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Gram Models\n",
    "\n",
    "To begin, we first chose to model our problem uaing several N-Gram models. This is a rather simple approach to word prediction since we can store an arbitrary number of N-Grams and predict words based on the previous N - 1 words. To be more specific, we limited our N-Gram models to trigrams, bigrams, and unigrams which means we can predict words based on 2 or less previous words. The predicted word is the N-Gram that has the highest probability with that word as the last item and the previous items being the previous words, if any. The creation and preparation of such N-Grams for our models can be handled by NLTK's Everygram Preprocessor. The models come from NLTK's LanguageModels which we decided to use because of the ease of use in terms of input and output for our tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Likelihood Estimator (MLE)\n",
    "\n",
    "The MLE model serves as the basis for our N-Gram modeling. It utilizes the algorithm described above without any smoothing or extra preparation. The model's only concern is the raw likelihoods for the word predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MLE model...\n"
     ]
    }
   ],
   "source": [
    "from nltk.lm import MLE\n",
    "\n",
    "mle = MLE(3)\n",
    "\n",
    "print(\"Training MLE model...\")\n",
    "train, vocab = nltk.lm.preprocessing.padded_everygram_pipeline(3, sentence_tokens)\n",
    "mle.fit(train, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laplace\n",
    "\n",
    "The Laplace model utilizes the MLE model while also implemeting add-1 smoothing. This leads to more accurate word probabilities in general since we can assign non-zero probabilities to unseen words. We are using this model as a direct comparison to the MLE model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Laplace model...\n"
     ]
    }
   ],
   "source": [
    "from nltk.lm import Laplace\n",
    "\n",
    "laplace = Laplace(3)\n",
    "\n",
    "print(\"Training Laplace model...\")\n",
    "train, vocab = nltk.lm.preprocessing.padded_everygram_pipeline(3, sentence_tokens)\n",
    "laplace.fit(train, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lidstone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Lidstone model is the same as the Laplace model but instead of add-1 smoothing, we can specify the amount of smoothing. We chose to create three different Lidstone models, initialized with add-0.25 smoothing, add-0.5 smoothing, and add-0.75 smoothing, respectively. We are using this model to demonstrate how smoothing affects the word predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Lidstone models...\n"
     ]
    }
   ],
   "source": [
    "from nltk.lm import Lidstone\n",
    "\n",
    "lidstone_25 = Lidstone(0.25, 3)\n",
    "lidstone_50 = Lidstone(0.5, 3)\n",
    "lidstone_75 = Lidstone(0.75, 3)\n",
    "\n",
    "print(\"Training Lidstone models...\")\n",
    "train, vocab = nltk.lm.preprocessing.padded_everygram_pipeline(3, sentence_tokens)\n",
    "lidstone_25.fit(train, vocab)\n",
    "train, vocab = nltk.lm.preprocessing.padded_everygram_pipeline(3, sentence_tokens)\n",
    "lidstone_50.fit(train, vocab)\n",
    "train, vocab = nltk.lm.preprocessing.padded_everygram_pipeline(3, sentence_tokens)\n",
    "lidstone_75.fit(train, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stupid Backoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Stupid Backoff model utilizes the MLE model while also providing the ability to scale lower order probabilities. The downside of this is that it is not a true probability distribution. We chose to create three different Stupid Backoff models, initialized with 0.25, 0.5, and 0.75, respectively. We are using this model to determine at what degree lower order probabilities affect the word predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Stupid Backoff models...\n"
     ]
    }
   ],
   "source": [
    "from nltk.lm import StupidBackoff\n",
    "\n",
    "stupid_backoff_25 = StupidBackoff(0.25, 3)\n",
    "stupid_backoff_50 = StupidBackoff(0.5, 3)\n",
    "stupid_backoff_75 = StupidBackoff(0.75, 3)\n",
    "\n",
    "print(\"Training Stupid Backoff models...\")\n",
    "train, vocab = nltk.lm.preprocessing.padded_everygram_pipeline(3, sentence_tokens)\n",
    "stupid_backoff_25.fit(train, vocab)\n",
    "train, vocab = nltk.lm.preprocessing.padded_everygram_pipeline(3, sentence_tokens)\n",
    "stupid_backoff_50.fit(train, vocab)\n",
    "train, vocab = nltk.lm.preprocessing.padded_everygram_pipeline(3, sentence_tokens)\n",
    "stupid_backoff_75.fit(train, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion and Future Work"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
