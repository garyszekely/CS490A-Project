{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word prediction recently has become a very integral part of everyday lives as mobile devices become more accessible to people everyday. In this paper we will tackle the topic of building such a piece of software to predict words so we can deepen our understanding of the technology and see how it can be beneficial to the everyday person as well as to the field of augmentative and alternative communications (AAC). We used two models to test our predictor, an N-Grams based model and a LSTM based model. These our integral to our approach since we need models that can \"remember\" words that previously came before the word currently being predicted. We use this information to then calculate probabilities for the next possible word until the end of a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Related Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization & Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization and normalization is very important as we need to ensure that our NLP model is not skewed by unclean data. Our first step in tokenization is to separate our training Amazon reviews into a list of sentence tokens using NLTK's sentence tokenizer. Finally, separate this list of sentence tokens into a list of lists of word tokens using NLTK's TweetTokenizer. As for normalization, we chose to lowercase all the word tokens as to get more meaningful results not altered by capitalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing and normalizing...\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import csv\n",
    "\n",
    "sentence_tokens = []\n",
    "tokenizer = nltk.tokenize.TweetTokenizer()\n",
    "\n",
    "with open('../data/sample.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    print(\"Tokenizing and normalizing...\")\n",
    "    for row in csv_reader:\n",
    "        sentence_tokens += [tokenizer.tokenize(sentence.lower()) for sentence in nltk.sent_tokenize(row[2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Training Sentence Tokens: 470931\n",
      "Average Number of Training Word Tokens per Training Sentence Token: 18.113396654711625\n",
      "Example Training Sentence Token: ['my', 'lovely', 'pat', 'has', 'one', 'of', 'the', 'great', 'voices', 'of', 'her', 'generation', '.']\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "print(\"Total Training Sentence Tokens:\", len(sentence_tokens))\n",
    "print(\"Average Number of Training Word Tokens per Training Sentence Token:\", statistics.mean(map(lambda e: len(e), sentence_tokens)))\n",
    "print(\"Example Training Sentence Token:\", sentence_tokens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Grams Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, we first chose to model our problem using N-Grams. This is a rather simple approach to word prediction since we can store an arbitrary number of N-Grams and predict words based on the previous N - 1 words. To be more specific, we limited our N-Grams model to trigrams, bigrams, and unigrams which means we can predict words based on 2 or less previous words. The predicted word is the N-Gram that has the highest probability with that word as the last item and the previous items being the previous words, if any. This can all be handled by NLTK's Everygram Preprocessor (to create N-Grams) and NLTK's Most Liklihood Estimator (to calculate the probabilities to make a prediction). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "lm = nltk.lm.MLE(3)\n",
    "train, vocab = nltk.lm.preprocessing.padded_everygram_pipeline(3, sentence_tokens)\n",
    "print(\"Training the model...\")\n",
    "lm.fit(train, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion and Future Work"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
